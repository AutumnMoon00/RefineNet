{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import laspy\n",
    "import numpy as np\n",
    "from scipy.spatial import KDTree\n",
    "from user_inputs import laz_file_path, clip_folder_path\n",
    "from user_inputs import search_radius, nn_threshold, bin_count_nn\n",
    "from helper_functions import generate_bins, fraction_of_trues\n",
    "import time\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'D:\\\\Geomatics\\\\thesis\\\\RefineNet\\\\data\\\\AHN\\\\clipped\\\\C_37EN2_clipped_8th_part.LAZ'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "laz_file_name = laz_file_path.split(\"\\\\\")[-1].split(\".\")\n",
    "clip_laz_file_name = laz_file_name[0] + \"_clipped_8th_part.\" + laz_file_name[1]\n",
    "clip_laz_path = clip_folder_path + \"\\\\\" + clip_laz_file_name\n",
    "clip_laz_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "laz = laspy.read(clip_laz_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of points: 18742173\n"
     ]
    }
   ],
   "source": [
    "# number of points\n",
    "print(f\"number of points: {laz.header.point_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X\n",
      "Y\n",
      "Z\n",
      "intensity\n",
      "return_number\n",
      "number_of_returns\n",
      "scan_direction_flag\n",
      "edge_of_flight_line\n",
      "classification\n",
      "synthetic\n",
      "key_point\n",
      "withheld\n",
      "scan_angle_rank\n",
      "user_data\n",
      "point_source_id\n",
      "gps_time\n",
      "Amplitude\n",
      "Reflectance\n",
      "Deviation\n",
      "\n",
      "unique classifications in clipped region:  [ 1  2  6  9 26]\n"
     ]
    }
   ],
   "source": [
    "# printing out all the available dimensions in clipped laz file\n",
    "for dimension in laz.point_format.dimensions:\n",
    "    print(dimension.name)\n",
    "print(\"\\nunique classifications in clipped region: \", np.unique(laz.classification))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Neighbourhood search steps**\n",
    "- making a kd tree (first load the points)\n",
    "- finding the nearest points within 0.5 m radius\n",
    "    - we can first build np array (or pandas dataframe) with all the required points, classification (labels), rgb(?), intensities\n",
    "    - then use np.where to find out the distances and indexes of the neighbour points\n",
    "    - filter only those within 0.5m and look for their classifications\n",
    "    - give confidence scores to points\n",
    "    - add extra dimension to laz file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xyz shape:  (18742173, 3)\n"
     ]
    }
   ],
   "source": [
    "# loading the xyz values\n",
    "xyz = laz.xyz.copy()\n",
    "print(\"xyz shape: \", xyz.shape)\n",
    "# build a kd tree\n",
    "tree = KDTree(xyz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels shape:  (18742173,)\n"
     ]
    }
   ],
   "source": [
    "# loading the classifications of points\n",
    "labels = laz.classification\n",
    "print(\"labels shape: \", labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Binning the data to take off load from kd tree**\n",
    "\n",
    "- number of bins is a hyper-parameter, can be modified in `user_inputs.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_points:  18742173\n",
      "number of bins:  40\n",
      "[[0, 468554], [468554, 937109], [937109, 1405663], [1405663, 1874217], [1874217, 2342772], [2342772, 2811326], [2811326, 3279880], [3279880, 3748435], [3748435, 4216989], [4216989, 4685543], [4685543, 5154098], [5154098, 5622652], [5622652, 6091206], [6091206, 6559761], [6559761, 7028315], [7028315, 7496869], [7496869, 7965424], [7965424, 8433978], [8433978, 8902532], [8902532, 9371086], [9371086, 9839641], [9839641, 10308195], [10308195, 10776749], [10776749, 11245304], [11245304, 11713858], [11713858, 12182412], [12182412, 12650967], [12650967, 13119521], [13119521, 13588075], [13588075, 14056630], [14056630, 14525184], [14525184, 14993738], [14993738, 15462293], [15462293, 15930847], [15930847, 16399401], [16399401, 16867956], [16867956, 17336510], [17336510, 17805064], [17805064, 18273619], [18273619, 18742173]]\n"
     ]
    }
   ],
   "source": [
    "# binning for neighbourhood search, nobebook crashing when given all the points\n",
    "num_points = laz.header.point_count\n",
    "nn_bins = generate_bins(number=num_points, bin_count=bin_count_nn)\n",
    "print(\"num_points: \", num_points)\n",
    "print(\"number of bins: \", bin_count_nn)\n",
    "print(nn_bins)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *Looping throung all the bins sequentially and calculate confidences*  \n",
    "- **Get all the neighbouring points (including itself) for a given point**\n",
    "- seperate the current point from the neighbours\n",
    "- get classification of the current point\n",
    "- get classification of the neighbour points\n",
    "- **Calculate confidences**\n",
    "    - **count the percentage of neighbour points are of same classification**\n",
    "    - **points with neighbours < threshold => we would be giving `0` confidence.. should be altered later**\n",
    "\n",
    "**Confidence = (label_match_count) / (neighbour_count)**\n",
    "1. get match count\n",
    "2. get neighbour count\n",
    "3. calculate confidence\n",
    "\n",
    "**After**\n",
    "1. get threshold mask\n",
    "2. less than 5 threshold number of neighbours, give confidence of `0`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing bins:   0%|          | 0/40 [00:00<?, ?it/s]C:\\conda_tmp\\ipykernel_25928\\394567318.py:67: RuntimeWarning: invalid value encountered in divide\n",
      "  confidences = match_pts_nn_num / nn_count\n",
      "Processing bins: 100%|██████████| 40/40 [03:13<00:00,  4.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18742173, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "confidence_scores = []\n",
    "count = 0\n",
    "point_count = 0\n",
    "for bin in tqdm(nn_bins, desc=\"Processing bins\"):\n",
    "\n",
    "    # if count != 15:\n",
    "    #     count += 1\n",
    "    #     continue\n",
    "    # else:\n",
    "    #     count += 1\n",
    "\n",
    "    # the query points numbers and their labels    \n",
    "    start, end = bin[0], bin[1]\n",
    "    pts = np.arange(start, end, step=1)\n",
    "    pts_vector = pts.reshape((-1, 1))\n",
    "    pts_labels = np.array(labels[pts])\n",
    "    pts_labels = pts_labels.reshape((-1, 1))  # reshaping it to a column vector -> easy to make comparison with neighbours\n",
    "\n",
    "    # neighbours information of query points\n",
    "    nn = tree.query_ball_point(xyz[start:end, :], r=0.5, p=2, workers=-1, return_sorted=True)\n",
    "\n",
    "    #   # should vectorize calculations so find the query point with max number of elements\n",
    "    max_length = max(len(lst) for lst in nn)\n",
    "    nn_np_padded = np.array([lst + [np.nan] * (max_length - len(lst)) for lst in nn], dtype=object)  # self points should be removed\n",
    "    nn_np_padded_float = nn_np_padded.astype(float)  # self points should be removed\n",
    "\n",
    "    # print(\"pts_vector[250]: \", pts_vector[250])\n",
    "    # print(\"nn_np_padded[250]: \", nn_np_padded[250])\n",
    "\n",
    "    # remove the search point itself from the neighbours by creating a mask\n",
    "    self_mask = (pts_vector == nn_np_padded_float)\n",
    "    nn_np_padded[self_mask] = np.nan\n",
    "    nn_np_padded_float[self_mask] = np.nan\n",
    "\n",
    "    # print(\"nn_np_padded[250]: \", nn_np_padded[250])\n",
    "\n",
    "    # flattening the neighbours array \n",
    "    # so that it is easy to pass to labels and get the classificaitons\n",
    "    nn_np_padded_flat = nn_np_padded.reshape((1, -1))\n",
    "    nn_np_padded_float_flat = nn_np_padded_float.reshape((1, -1))\n",
    "    # nanMask ## True when the element is nan\n",
    "    nn_np_padded_float_flat_nanMask = np.isnan(nn_np_padded_float_flat)\n",
    "    nn_np_padded_Mask = ~np.isnan(nn_np_padded_float)\n",
    "\n",
    "    # replacing all the nan's with -1 index as a placeholder, later we can use mask again to filter them out \n",
    "    nn_np_padded_flat[nn_np_padded_float_flat_nanMask] = -1  # replacing all the nan's with -1, now we can use this variable to get neighbours classifications\n",
    "    nn_labels_flat_pHold = labels[list(nn_np_padded_flat[0])]\n",
    "    nn_labels_flat_pHold = np.array(nn_labels_flat_pHold)\n",
    "    # nn_labels_flat = nn_labels_flat_pHold *  ~nn_np_padded_float_flat_nanMask\n",
    "    nn_labels_flat = nn_labels_flat_pHold.copy()\n",
    "    nn_labels_flat_float = nn_labels_flat.astype(float)\n",
    "    nn_labels_flat_float[nn_np_padded_float_flat_nanMask[0]] = np.nan\n",
    "\n",
    "    # 2d array => no more flattened\n",
    "    nn_labels_float = nn_labels_flat_float.reshape((nn_np_padded.shape[0], -1))\n",
    "\n",
    "\n",
    "    # 1. compare pts_labels with nn_labels\n",
    "    match_pts_nn_bool = (pts_labels == nn_labels_float)\n",
    "    match_pts_nn_num = np.sum(match_pts_nn_bool, axis=1, keepdims=True) # number of points that match\n",
    "\n",
    "    # 2. nn_count, create nn mask to filter out the points which are not surrounded by many points\n",
    "    nn_count = np.sum(nn_np_padded_Mask, axis=1, keepdims=True)\n",
    "    nn_threshold_mask = (nn_count < nn_threshold)\n",
    "\n",
    "    # 3. calculating the confidences\n",
    "    confidences = match_pts_nn_num / nn_count \n",
    "    confidences[nn_threshold_mask] = 0\n",
    "    \n",
    "    # 4. combining all the bins together\n",
    "    confidence_scores.append(confidences)\n",
    "\n",
    "    # print(\"start, end: \", start, end)\n",
    "    # print(\"pts_vector.shape: \", pts_vector.shape)\n",
    "    # print(\"pts_vector: \", pts_vector)\n",
    "    # print(\"nn.shape: \", nn.shape)\n",
    "    # print()\n",
    "\n",
    "\n",
    "confidence_scores = np.vstack(confidence_scores)\n",
    "print(confidence_scores.shape)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1. ],\n",
       "       [1. ],\n",
       "       [1. ],\n",
       "       ...,\n",
       "       [1. ],\n",
       "       [0.9],\n",
       "       [1. ]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confidence_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Useless"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confidence_scores = []\n",
    "count = 0\n",
    "point_count = 0\n",
    "\n",
    "for bin in tqdm(nn_bins, desc=\"Processing bins\"):\n",
    "\n",
    "    # the query points numbers and their labels \n",
    "    start, end = bin[0], bin[1]\n",
    "    pts = np.arange(start, end, step=1)\n",
    "    pts_vector = pts.reshape((-1, 1))\n",
    "    pts_labels = np.array(labels[pts])\n",
    "    pts_labels = pts_labels.reshape((-1, 1))  # reshaping it to a column vector -> easy to make comparison with neighbours\n",
    "\n",
    "    point_count += pts_vector.shape[0]\n",
    "\n",
    "\n",
    "    # # neighbours information of query points\n",
    "    # nn = tree.query_ball_point(xyz[start:end, :], r=0.5, p=2, workers=-1, return_sorted=True)\n",
    "\n",
    "    # #   # should vectorize calculations so find the query point with max number of elements\n",
    "    # max_length = max(len(lst) for lst in nn)\n",
    "    # nn_np_padded = np.array([lst + [np.nan] * (max_length - len(lst)) for lst in nn], dtype=object)  # self points should be removed\n",
    "    # nn_np_padded_float = nn_np_padded.astype(float)  # self points should be removed\n",
    "\n",
    "    # print(\"pts_vector[250]: \", pts_vector[250])\n",
    "    # print(\"nn_np_padded[250]: \", nn_np_padded[250])\n",
    "\n",
    "    # # remove the search point itself from the neighbours by creating a mask\n",
    "    # self_mask = (pts_vector == nn_np_padded_float)\n",
    "    # nn_np_padded[self_mask] = np.nan\n",
    "    # nn_np_padded_float[self_mask] = np.nan\n",
    "\n",
    "    # print(\"nn_np_padded[250]: \", nn_np_padded[250])\n",
    "\n",
    "    # # flattening the neighbours array \n",
    "    # # so that it is easy to pass to labels and get the classificaitons\n",
    "    # nn_np_padded_flat = nn_np_padded.reshape((1, -1))\n",
    "    # nn_np_padded_float_flat = nn_np_padded_float.reshape((1, -1))\n",
    "    # # nanMask ## True when the element is nan\n",
    "    # nn_np_padded_float_flat_nanMask = np.isnan(nn_np_padded_float_flat)\n",
    "    # nn_np_padded_Mask = ~np.isnan(nn_np_padded_float)\n",
    "\n",
    "    # # replacing all the nan's with -1 index as a placeholder, later we can use mask again to filter them out \n",
    "    # nn_np_padded_flat[nn_np_padded_float_flat_nanMask] = -1  # replacing all the nan's with -1, now we can use this variable to get neighbours classifications\n",
    "    # nn_labels_flat_pHold = labels[list(nn_np_padded_flat[0])]\n",
    "    # nn_labels_flat_pHold = np.array(nn_labels_flat_pHold)\n",
    "    # # nn_labels_flat = nn_labels_flat_pHold *  ~nn_np_padded_float_flat_nanMask\n",
    "    # nn_labels_flat = nn_labels_flat_pHold.copy()\n",
    "    # nn_labels_flat_float = nn_labels_flat.astype(float)\n",
    "    # nn_labels_flat_float[nn_np_padded_float_flat_nanMask[0]] = np.nan\n",
    "\n",
    "    # # 2d array => no more flattened\n",
    "    # nn_labels_float = nn_labels_flat_float.reshape((nn_np_padded.shape[0], -1))\n",
    "\n",
    "\n",
    "    # # 1. compare pts_labels with nn_labels\n",
    "    # match_pts_nn_bool = (pts_labels == nn_labels_float)\n",
    "    # match_pts_nn_num = np.sum(match_pts_nn_bool, axis=1, keepdims=True) # number of points that match\n",
    "\n",
    "    # # 2. nn_count, create nn mask to filter out the points which are not surrounded by many points\n",
    "    # nn_count = np.sum(nn_np_padded_Mask, axis=1, keepdims=True)\n",
    "    # nn_threshold_mask = (nn_count < nn_threshold)\n",
    "\n",
    "    # # 3. calculating the confidences\n",
    "    # confidences = match_pts_nn_num / nn_count \n",
    "    # confidences[nn_threshold_mask] = 0\n",
    "    \n",
    "\n",
    "\n",
    "    # print(\"start, end: \", start, end)\n",
    "    # print(\"pts_vector.shape: \", pts_vector.shape)\n",
    "    # print(\"pts_vector: \", pts_vector)\n",
    "    # print(\"nn.shape: \", nn.shape)\n",
    "    # # print()\n",
    "\n",
    "\n",
    "    # \n",
    "print(f\"point_count: {point_count}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "laz.add_extra_dim(laspy.ExtraBytesParams(\n",
    "    name=\"confidence\",\n",
    "    type=np.float64,\n",
    "    description=\"Confidence of points\"\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X\n",
      "Y\n",
      "Z\n",
      "intensity\n",
      "return_number\n",
      "number_of_returns\n",
      "scan_direction_flag\n",
      "edge_of_flight_line\n",
      "classification\n",
      "synthetic\n",
      "key_point\n",
      "withheld\n",
      "scan_angle_rank\n",
      "user_data\n",
      "point_source_id\n",
      "gps_time\n",
      "Amplitude\n",
      "Reflectance\n",
      "Deviation\n",
      "confidence\n"
     ]
    }
   ],
   "source": [
    "# printing out all the available dimensions in clipped laz file\n",
    "for dimension in laz.point_format.dimensions:\n",
    "    print(dimension.name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "laz.confidence = confidence_scores.reshape((confidence_scores.shape[0], ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1. , 1. , 1. , ..., 1. , 0.9, 1. ])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "laz.confidence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## outputting the file with confidences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'D:\\\\Geomatics\\\\thesis\\\\RefineNet\\\\data\\\\AHN\\\\clipped\\\\C_37EN2_clipped_8th_part_confidences.LAZ'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confidence_clip_laz_file_name = laz_file_name[0] + \"_clipped_8th_part_confidences.\" + laz_file_name[1]\n",
    "confidence_clip_laz_path = clip_folder_path + \"\\\\\" + confidence_clip_laz_file_name\n",
    "confidence_clip_laz_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "LaspyException",
     "evalue": "You are using laspy 2.0, which has several improvements over 1.x\n            but with several breaking changes.\n            To stay on laspy 1.x: `pip install laspy<2.0.0`\n            \n            In short:\n              - To read a file do: las = laspy.read('somefile.laz')\n              - To create a new LAS data do: las = laspy.create(point_format=2, file_version='1.2')\n              - To write a file previously read or created: las.write('somepath.las')\n            See the documentation for more information about the changes https://laspy.readthedocs.io/en/latest/",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLaspyException\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32md:\\Geomatics\\thesis\\RefineNet\\codes\\point_cloud\\nSearch_v2.ipynb Cell 22\u001b[0m line \u001b[0;36m4\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Geomatics/thesis/RefineNet/codes/point_cloud/nSearch_v2.ipynb#X31sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# outputting the file\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Geomatics/thesis/RefineNet/codes/point_cloud/nSearch_v2.ipynb#X31sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39m# laz_with_confidence = \u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Geomatics/thesis/RefineNet/codes/point_cloud/nSearch_v2.ipynb#X31sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39m# Create a new LAS/LAZ file for writing\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/Geomatics/thesis/RefineNet/codes/point_cloud/nSearch_v2.ipynb#X31sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m laz_with_confidence \u001b[39m=\u001b[39m laspy\u001b[39m.\u001b[39;49mfile\u001b[39m.\u001b[39;49mFile(confidence_clip_laz_path, mode\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mw\u001b[39;49m\u001b[39m\"\u001b[39;49m, header\u001b[39m=\u001b[39;49mlaz\u001b[39m.\u001b[39;49mheader)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Geomatics/thesis/RefineNet/codes/point_cloud/nSearch_v2.ipynb#X31sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39m# Copy existing header\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Geomatics/thesis/RefineNet/codes/point_cloud/nSearch_v2.ipynb#X31sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m laz_with_confidence\u001b[39m.\u001b[39mheader \u001b[39m=\u001b[39m laz\u001b[39m.\u001b[39mheader\n",
      "File \u001b[1;32md:\\Geomatics\\thesis\\RefineNet\\venv\\lib\\site-packages\\laspy\\file.py:6\u001b[0m, in \u001b[0;36mFile.__init__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m----> 6\u001b[0m     \u001b[39mraise\u001b[39;00m errors\u001b[39m.\u001b[39mLaspyException(\n\u001b[0;32m      7\u001b[0m \u001b[39m        \u001b[39m\u001b[39m\"\"\"You are using laspy 2.0, which has several improvements over 1.x\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[39m        but with several breaking changes.\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[39m        To stay on laspy 1.x: `pip install laspy<2.0.0`\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[39m        \u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[39m        In short:\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[39m          - To read a file do: las = laspy.read('somefile.laz')\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[39m          - To create a new LAS data do: las = laspy.create(point_format=2, file_version='1.2')\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[39m          - To write a file previously read or created: las.write('somepath.las')\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[39m        See the documentation for more information about the changes https://laspy.readthedocs.io/en/latest/\"\"\"\u001b[39;00m\n\u001b[0;32m     16\u001b[0m     )\n",
      "\u001b[1;31mLaspyException\u001b[0m: You are using laspy 2.0, which has several improvements over 1.x\n            but with several breaking changes.\n            To stay on laspy 1.x: `pip install laspy<2.0.0`\n            \n            In short:\n              - To read a file do: las = laspy.read('somefile.laz')\n              - To create a new LAS data do: las = laspy.create(point_format=2, file_version='1.2')\n              - To write a file previously read or created: las.write('somepath.las')\n            See the documentation for more information about the changes https://laspy.readthedocs.io/en/latest/"
     ]
    }
   ],
   "source": [
    "# outputting the file\n",
    "# laz_with_confidence = \n",
    "# Create a new LAS/LAZ file for writing\n",
    "laz_with_confidence = laspy.file.File(confidence_clip_laz_path, mode=\"w\", header=laz.header)\n",
    "\n",
    "# Copy existing header\n",
    "laz_with_confidence.header = laz.header\n",
    "laz_with_confidence.write_points(laz.points)\n",
    "laz_with_confidence.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "with laspy.open(confidence_clip_laz_path, mode=\"w\", header=laz.header) as writer:\n",
    "        # for points in laz.chunk_iterator(100000):\n",
    "    writer.write_points(laz.points)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18742173"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "laz.header.point_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
